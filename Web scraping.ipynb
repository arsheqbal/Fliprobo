{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all libraries\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading chrome driver\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping review of mobile phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=mobiles&ref=nb_sb_noss_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "product_url = []\n",
    "\n",
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping review of laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=laptops&ref=nb_sb_noss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping review of headphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=headphones&ref=nb_sb_noss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping reviews of smart watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=smart+watches&ref=nb_sb_noss_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping the review of cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=professional+cameras&ref=nb_sb_noss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the details of printers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=printers&ref=nb_sb_noss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the detail of monitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing url\n",
    "driver.get('https://www.amazon.in/s?k=monitors&ref=nb_sb_noss_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping the details of Home theatre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=home+theatre&crid=O1LQ8RRO83EY&sprefix=Home+thea%2Caps%2C375&ref=nb_sb_ss_ts-doa-p_1_9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping the detail of router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.amazon.in/s?k=router&crid=1JQT9E7UZI4OK&sprefix=router%2Caps%2C375&ref=nb_sb_ss_ts-doa-p_1_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "        product_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #storing the value of next page in name variable\n",
    "\n",
    "for i in range(19):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"): #storing next page product url\n",
    "        product_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath(\"//li[@class='a-last']/a\") #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the variable\n",
    "review = []\n",
    "rating = []\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # scraping rating\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='index-flexRow index-averageRating']\").text\n",
    "        rat = float(rat.split()[0])\n",
    "        if 0<=rat<=0.4:\n",
    "            rat = 0\n",
    "            rating.append(rat)\n",
    "        elif 0.5<=rat<=1.4:\n",
    "            rat = 1\n",
    "            rating.append(rat)\n",
    "        elif 1.5<=rat<=2.4:\n",
    "            rat = 2\n",
    "            rating.append(rat)\n",
    "        elif 2.5<=rat<=3.4:\n",
    "            rat = 3\n",
    "            rating.append(rat)\n",
    "        elif 3.5<=rat<=4.4:\n",
    "            rat = 4\n",
    "            rating.append(4)\n",
    "        elif 4.5<=rat<=5:\n",
    "            rat = 5\n",
    "            rating.append(rat)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "        \n",
    "    # scraping review\n",
    "    try:\n",
    "        rev = driver.find_element_by_xpath('//*[@id=\"detailedReviewsContainer\"]/div[1]/div[1]/div[2]')\n",
    "        review.append(rev.text)\n",
    "    except NoSuchElementException:\n",
    "        review.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.DataFrame({'Rating': rating, 'Review': review})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping detail from flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the url\n",
    "driver.get('https://www.flipkart.com/gaming/pr?sid=4rr&fm=neo%2Fmerchandising&iid=M_14dea359-b225-4706-8537-8e01d022e3bd_1_372UD5BXDFYS_MC.T6Z44HHCR56C&otracker=hp_rich_navigation_1_1.navigationCard.RICH_NAVIGATION_Electronics~Gaming~All_T6Z44HHCR56C&otracker1=hp_rich_navigation_PINNED_neo%2Fmerchandising_NA_NAV_EXPANDABLE_navigationCard_cc_1_L2_view-all&cid=T6Z44HHCR56C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipkart_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/mobile-accessories/power-banks/pr?sid=tyy,4mr,fu6&otracker=categorytree&otracker=nmenu_sub_Electronics_0_Power%20Banks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='s1Q9rs']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(23):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='s1Q9rs']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the url\n",
    "driver.get('https://www.flipkart.com/clothing-and-accessories/topwear/shirt/men-shirt/casual-shirt/pr?sid=clo,ash,axc,mmk,kp7&otracker=categorytree&otracker=nmenu_sub_Men_0_Casual%20Shirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/mobile-accessories/memory-cards-readers/memory-cards/pr?sid=tyy,4mr,zzf,7y7&otracker=nmenu_sub_Electronics_0_Memory%20Cards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=92.0.4515.107)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-e1a8b10490e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#passing the value of next page to the driver to connect to the next page.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mget_attribute\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mattributeValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w3c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             attributeValue = self.parent.execute_script(\n\u001b[0m\u001b[0;32m    140\u001b[0m                 \u001b[1;34m\"return (%s).apply(null, arguments);\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgetAttribute_js\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 self, name)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXECUTE_SCRIPT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         return self.execute(command, {\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             'args': converted_args})['value']\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=92.0.4515.107)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(12):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/wearable-smart-devices/smart-headphones/pr?sid=ajy,vam&otracker=nmenu_sub_Electronics_0_Smart%20Headphones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/mobile-accessories/cables/pr?sid=tyy,4mr,3nu&otracker=nmenu_sub_Electronics_0_Mobile%20Cables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the url\n",
    "driver.get('https://www.flipkart.com/mobile-accessories/chargers/pr?sid=tyy,4mr,tp2&otracker=nmenu_sub_Electronics_0_Mobile%20Chargers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the url\n",
    "driver.get('https://www.flipkart.com/mobile-accessories/mobile-holders/pr?sid=tyy,4mr,vnf&marketplace=FLIPKART&otracker=nmenu_sub_Electronics_0_Mobile%20Holders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(24):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the ur\n",
    "driver.get('https://www.flipkart.com/laptop-accessories/mouse/pr?sid=6bo,ai3,2ay&otracker=nmenu_sub_Electronics_0_Mouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(24):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing url\n",
    "driver.get('https://www.flipkart.com/audio-video/speakers/pr?count=40&otracker=categorytree&p%5B%5D=facets.type%255B%255D%3DHome%2BAudio%2BSpeaker&p%5B%5D=facets.type%255B%255D%3DLaptop%252FDesktop%2BSpeaker&p%5B%5D=facets.type%255B%255D%3DSoundbar&sid=0pm%2F0o7&otracker=nmenu_sub_Electronics_0_Home%20Audio%20Speakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the url\n",
    "driver.get('https://www.flipkart.com/iron/pr?sid=j9e%2Cabm%2Ca0u&otracker=nmenu_sub_Appliances_0_Irons&otracker=nmenu_sub_Appliances_0_Irons&otracker=nmenu_sub_TVs%20%26%20Appliances_0_Irons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/home-kitchen/home-appliances/fans/pr?sid=j9e,abm,lbz&otracker=categorytree&otracker=nmenu_sub_TVs%20%26%20Appliances_0_Fans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/home-kitchen/home-appliances/air-coolers/pr?sid=j9e,abm,52j&otracker=categorytree&otracker=nmenu_sub_TVs%20%26%20Appliances_0_Air%20Coolers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the url\n",
    "driver.get('https://www.flipkart.com/home-kitchen/home-appliances/voltage-stabilizers/pr?sid=j9e,abm,xf4&otracker=categorytree&otracker=nmenu_sub_TVs%20%26%20Appliances_0_Voltage%20Stabilizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching product url completed :-)\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=92.0.4515.107)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-197-ac678c596816>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#passing the value of next page to the driver to connect to the next page.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mget_attribute\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mattributeValue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_w3c\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             attributeValue = self.parent.execute_script(\n\u001b[0m\u001b[0;32m    140\u001b[0m                 \u001b[1;34m\"return (%s).apply(null, arguments);\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgetAttribute_js\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 self, name)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXECUTE_SCRIPT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         return self.execute(command, {\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             'args': converted_args})['value']\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: element is not attached to the page document\n  (Session info: chrome=92.0.4515.107)\n"
     ]
    }
   ],
   "source": [
    "# first we scrap all the product url from page 1.\n",
    "for a in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"):\n",
    "        flipkart_url.append(a.get_attribute('href'))\n",
    "        \n",
    "name = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]') #storing the value of next page in name variable\n",
    "\n",
    "for i in range(25):\n",
    "    driver.get(name.get_attribute('href')) #passing the value of next page to the driver to connect to the next page.\n",
    "    time.sleep(3)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath(\"//a[@class='_2rpwqI']\"): #storing next page product url\n",
    "        flipkart_url.append(j.get_attribute('href'))\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]') #now we again taking the url of next page.\n",
    "    except (StaleElementReferenceException, NoSuchElementException):\n",
    "        print(\"Fetching product url completed :-)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning the variable\n",
    "flipkart_review = []\n",
    "flipkart_rating = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total ratings extracted: 5251\r"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "\n",
    "for i in flipkart_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # scraping rating\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='_2d4LTz']\").text\n",
    "        rat = float(rat.split()[0])\n",
    "        if 0<=rat<=0.4:\n",
    "            rat = 0\n",
    "            flipkart_rating.append(rat)\n",
    "        elif 0.5<=rat<=1.4:\n",
    "            rat = 1\n",
    "            flipkart_rating.append(rat)\n",
    "        elif 1.5<=rat<=2.4:\n",
    "            rat = 2\n",
    "            flipkart_rating.append(rat)\n",
    "        elif 2.5<=rat<=3.4:\n",
    "            rat = 3\n",
    "            flipkart_rating.append(rat)\n",
    "        elif 3.5<=rat<=4.4:\n",
    "            rat = 4\n",
    "            flipkart_rating.append(4)\n",
    "        elif 4.5<=rat<=5:\n",
    "            rat = 5\n",
    "            flipkart_rating.append(rat)\n",
    "    except NoSuchElementException:\n",
    "        flipkart_rating.append('Null')\n",
    "        \n",
    "    # scraping review\n",
    "    try:\n",
    "        rev = driver.find_element_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        flipkart_review.append(rev.text)\n",
    "    except NoSuchElementException:\n",
    "        flipkart_review.append('Null')\n",
    "    print(\" Total ratings extracted: {}\".format(t), end='\\r')\n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Classy product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Decent product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Delightful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Good quality product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12611</th>\n",
       "      <td>3</td>\n",
       "      <td>Decent product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12612</th>\n",
       "      <td>3</td>\n",
       "      <td>Hated it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12613</th>\n",
       "      <td>4</td>\n",
       "      <td>Terrific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12614</th>\n",
       "      <td>4</td>\n",
       "      <td>Could be way better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615</th>\n",
       "      <td>4</td>\n",
       "      <td>Unsatisfactory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12616 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating                Review\n",
       "0          4        Classy product\n",
       "1          4        Decent product\n",
       "2          4            Delightful\n",
       "3          4             Wonderful\n",
       "4          4  Good quality product\n",
       "...      ...                   ...\n",
       "12611      3        Decent product\n",
       "12612      3             Hated it!\n",
       "12613      4              Terrific\n",
       "12614      4   Could be way better\n",
       "12615      4        Unsatisfactory\n",
       "\n",
       "[12616 rows x 2 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipkart = pd.DataFrame({'Rating': flipkart_rating, 'Review': flipkart_review})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging the amazon and flipkart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [amazon, flipkart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Love This Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great phone service and options, lousy case!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rating                                        Review\n",
       "0      3                   Def not best, but not worst\n",
       "1      1                   Text Messaging Doesn't Work\n",
       "2      5                               Love This Phone\n",
       "3      3                       Love the Phone, BUT...!\n",
       "4      4  Great phone service and options, lousy case!"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
